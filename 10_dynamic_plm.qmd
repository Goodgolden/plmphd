---
title: "Dynamic PLM (Height)"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warnings = FALSE,
                      message = FALSE,
                      comment = "#>",
                      #results = "hide",
                      digits = 4,
                      error = FALSE)

## clean the R environment
# graphics.off()
# rm(list = ls())
# freshr::freshr()

## load packages
library(here, quietly = TRUE)
library(lme4, quietly = TRUE)
library(devtools, quietly = TRUE)
# library(optimx, quietly = TRUE)

devtools::load_all()
## check the directory for the file
# here::dr_here()
# here::set_here()

## the figure or results should be saved 
# paste0("foldername/Sfilename_workingresult_", 
#      Sys.Date(), ".filetype")
```

```{css, echo=FALSE}
.scroll-100 {
  max-height: 300px;
  max-width: 1000px;
  overflow-y: auto;
  background-color: inherit;
}
```

## Goal for this chapter

-   [x] to check the brokenstick model impute `bks_plots`

-   [x] to confirm the linear model is the problem `lm_plots`

-   [x] to merge the brokenstick and linear into one step

    -   [x] using dynamic prediction MC `JMbayes::IndvPred_lme()`

    -   [x] writing a new function `plm2024::IndvPred_lmer()`, and it works!!!

    -   [x] comparing it with the `bks_plots` and `lm_plots`

    -   [x] gamlss model for all model

## Dynamic prediction Overall

```{r}
load("~/Desktop/plmphd/R/train_test.rda")
train1 <- train %>%
  group_by(id) %>%
  summarise(ht0 = min(ht),
            age0 = min(age))

train2 <- full_join(train, train1, by = "id") %>%
  mutate(sid = "S") %>%
  unite(id, sid, id, sep = "") %>% 
  mutate(sex = as.factor(sex))

test1 <- test %>%
  group_by(id) %>%
  summarise(ht0 = min(ht),
            age0 = min(age))

test2 <- full_join(test, test1, by = "id") %>%
  mutate(sid = "S") %>%
  unite(id, sid, id, sep = "") %>% 
  mutate(sex = as.factor(sex))
```

### Results for dynamic prediction (Archived)

Because the brokenstick model is too simple with any other covariates, 
the performance is terrible. The imputations are shrinking towards the marginal values.

```{r}
meanout <- function(dataset){
  result0 <- dataset %>%
    as.data.frame() %>%
    mutate(mse = bias^2)
  result1 <- result0 %>%
    colMeans() %>%
    unlist()
  return(result1)
}
```


## Step1 Brokenstick

Predict every time points for each individual, and then impute the missing values.
anchor time sets get to extreme levels with every 0.5 step.

Fitting the model with both `nlme::lme()` and `lme4::lmer()`.

```{r}
bsk_knots <- c(5, 10, 15)
anchor <- c(8, 10, 12)

# mlmf <- "ht ~ sex + sex:ht0 + 
#               bmi + bmi:ht0 + 
#               age + age:ht0 + 
#               genotype + genotype:ht0 + 
#               ethnic + ethnic:ht0 + ht0"

# lmf <- "ht ~ as.factor(time) + 
#               sex + sex:ht0 + sex:t0 + 
#               bmi + bmi:ht0 + bmi:t0 +
#               age + age:ht0 + age:t0 +
#               genotype + genotype:ht0 + genotype:t0 +
#               ethnic + ethnic:ht0 + ethnic:t0 +
#               ht0 + t0"
```

### `nlme::lme()` takes too long time

Stef is right, the `nlme::lme()` takes way too long time. Also having the convergence problem with more than six internal knots.

Need to rewrite the `IndvPred_lme()` into the `IndvPred_lmer()`. Probably change the name to as `plm2024::dyn_impute`

```         
Quitting from lines 99-123 [unnamed-chunk-2] (25_check.qmd)
Error in `lme.formula()`:
! optim problem, convergence error code = 1
  message = 
Backtrace:
 1. nlme::lme(...)
 2. nlme::lme.formula(...)
```

### Writing my own function for `IndvPred_lmer()`

The Drizopoulos `JMbayes2` package provide only the `nlme::lme()` function pull out results.

Benchmark results for this run with and without derivatives show an approximately 20% speedup. 
This is a case with only 2 top-level parameters, 
but the fit took only 31 deviance function evaluations (see `m0@optinfo$feval`) to converge, 
so the effect of the additional $7 (n2âˆ’n+1)$ function evaluations is noticeable.

```{r}
# ctl <- .makeCC("warning", tol = 1e-3)
bks_lmer <- lmer(ht ~ 1 + bs(time, knots = c(3, 6, 9, 12), degree = 1) * sex + 
                   (1 + bs(time, knots = c(3, 6, 9, 12), degree = 1) * sex| id),
                 # na.action = na.exclude,
                 control = lmerControl(check.nobs.vs.nRE = "warning",
                                         optCtrl = list(method = 'nlminb'),
                                         optimizer = "optimx"),
                 data = train2)

# bks <- brokenstick::brokenstick(ht ~ time | id, 
#                    data = train2, 
#                    method = "lmer",
#                    knots = c(4, 8, 12), 
#                    seed = 123)
summary(bks_lmer)
summary(rePCA(bks_lmer))
# 
# bks_imput <- impute_brokenstick(outcome_var = "ht",
#                                 time_var = "time",
#                                 id_var = "id",
#                                 bs_knots = bsk_knots,
#                                 anchor_time = anchor,
#                                 data = train2)
save(bks_lmer, file = "results/P01_brokenstick_lmer_model.Rdata")


```

You can check with the rePCA function from 
lme4 with the following code: summary(rePCA(model)). 
The result will provide a table that provides the proportion of 
variance explained in your random effect structure.
If you have any column that explains near 0 proportion of variance,
this is likely the issue and is causing the singular fit error. 
Another way to confirm this, is to plot out 
the mean RT for each condition by participant to
create what's called a "spaghetti plot". 

Add lines and group them by the participant so that you can see the changes in 
RTs for each condition by the participant. 
If the lines are all parallel (or nearly parallel), 
then this would indicate that there is likely no randomness in 
your effect of condition.

## Step2 Imputation

The dynamic prediction will automatically add the initial values to
cause trouble remember to remove all of them.

```{r}
#| eval: false
 
test_baseline <- test2 %>%
  group_by(id) %>%
  mutate(group_index = row_number()) %>% 
  filter(time <= 2)

## map the function to all individuals
# 1 + bs(time, knots = c(3, 6, 9, 12), degree = 1) * sex 
# (1 + bs(time, knots = c(3, 6, 9, 12), degree = 1) + sex| id),
lp_test <- IndvPred_lmer(lmerObject = bks_lmer,
                          data = train2,
                          newdata = test_baseline,
                          timeVar = "time",
                          outcomeVar = "ht",
                          idVar = "id",
                          lmer.fixed = "1 + bs(time, knots = c(3, 6, 9, 12),
                         degree = 1) * sex",
                          lmer.random = "1 + bs(time, knots = c(3, 6, 9, 12), 
                         degree = 1) * sex",
                          M = 500,
                          times = anchor,
                          all_times = TRUE,
                          return_data = FALSE,
                          level = 0.9,
                          interval = "prediction",
                          seed = 555)
## drizopoulos code for building the data.predict
# extract_lmeComponents(bks_nlme, timeVar = "time")
# View(extract_lmeComponents)



```

```{r}
#| eval: false
#| fig-width: 50
#| fig-height: 60

lp_test <- lp_test %>%
  as.data.frame() %>%
  mutate(ht = as.numeric(predicted_y),
         ht = round(ht, 2))
plot2 <- ggplot() +
  # geom_line(data = data1,
  #           aes(x = as.numeric(time), 
  #               y = ht, 
  #               group = id),
  #           color = "indianred") +
  # geom_point(data = data1,
  #            aes(x = as.numeric(time), 
  #               y = ht, 
  #               group = id)) +
  geom_line(data = lp_test,
            aes(x = as.numeric(time), 
                y = ht, 
                group = id),
            color = "darkgreen",
            alpha = 0.3) +
  geom_point(data = lp_test,
             aes(x = as.numeric(time), 
                y = ht, 
                group = id)) +
  geom_line(data = test2,
            aes(x = time,
                y = ht,
                group = id),
            color = "grey") +
  facet_wrap(~id, ncol = 30) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(title = "Individual Trajectories",
       x = "Time",
       y = "Outcome Score")

plot2

ggsave("figures/P01_dynamic_predict_trajectories_time2.png",
       plot2,
       width = 45,
       height = 45,
       units = "in",
       dpi = 300)

```

### Dynamic prediction with different random effects

This is what we have when we do not have very complicated random effects, 
also the random effects is not the same the fixed effects,
somehow it is not working very well (very interesting cases). 

I have tried the model without any extra information with gender or
genotype but it does not work well. 

#### Simple Random effect without gender information

- lmer.fixed = "1 + bs(time, knots = c(3, 6, 9, 12), degree = 1)",
- lmer.random = "1 + bs(time, knots = c(3, 6, 9, 12), degree = 1)",

```{r}
knitr::include_graphics("figures/P01_dynamic_predict_trajectories_annotated00.png")
```


#### Random effect with extra random intercept of gender 

- lmer.fixed = "1 + bs(time, knots = c(3, 6, 9, 12), degree = 1) * sex"
- lmer.random = "1 + bs(time, knots = c(3, 6, 9, 12), degree = 1) + sex"

getting better than the first try but still not very well. 

```{r}
knitr::include_graphics("figures/P01_dynamic_predict_trajectories_annotated01.png")
```

#### Complicated Random effect the same as Fixed 

- lmer.fixed = "1 + bs(time, knots = c(3, 6, 9, 12), degree = 1) * sex ",
- lmer.random = "1 + bs(time, knots = c(3, 6, 9, 12), degree = 1) * sex",

```{r}
knitr::include_graphics("figures/P01_dynamic_predict_trajectories_annotated02.png")
```

Here is the comparison for brokenstick prediction and dynamic prediction. We hope that the dynamic prediction is much better than the brokenstick.



```{r}
knitr::include_graphics("figures/P01_dynamic_predict_trajectories_time2.png")
```

Now we have only the first two years of the data, and we can see it is better somehow.

need to systematically analysis this:

-   generate data with the anchor times in the training data

-   see how the error accumulate over each step

-   **through dynamic prediction is no so good**

```{r}
#| eval: false
#| fig-wideth: 40
#| fig-height: 60

train3 <- train2 %>% 
  group_by(id) %>%
  slice(1L) %>% 
  dplyr::select(-time) %>%
  ungroup()

names(train2); names(train3)

# anchor <- eval(parse(text = anchor))
lb_train0 <- expand.grid(time = anchor,
                        id = train3$id)
lb_train1 <- lb_train0 %>% 
  as.data.frame() %>%
  full_join(train3, by = c("id"))

lb_train2 <- lb_train0 %>%  
  cbind(bs(lb_train0$time, knots = c(3, 6, 9, 12), degree = 1)) %>%
  as.data.frame()

lb_train3 <- lb_train2 %>% 
  full_join(train3, by = c("id")) %>% 
  mutate(`1:sexM` = `1` * ifelse(sex == "M", 1, 0),
         `2:sexM` = `2` * ifelse(sex == "M", 1, 0),
         `3:sexM` = `3` * ifelse(sex == "M", 1, 0),
         `4:sexM` = `4` * ifelse(sex == "M", 1, 0),
         `5:sexM` = `5` * ifelse(sex == "M", 1, 0)) %>%
  mutate(intercept = 1,
         sex = ifelse(sex == "M", 1, 0)) %>%
  dplyr::select(intercept, `1`:`5`, sex ,`1:sexM`:`5:sexM`) %>%
  mutate_all(as.numeric) %>%
  as.matrix()



re <- ranef(bks_lmer)[["id"]] %>% as.data.frame() %>%
  slice(rep(1:n(), each = 3))

dim(re) # 913 12
dim(lb_train) # 2739 12
A <- rowSums(lb_train * re)
lb_train1$pred <- predict(bks_lmer,
                re.form = NA,
                newdata = lb_train1) + A

# lb_train$pred <- predict(bks_lmer, 
#                          re.form = ~(1 + time * sex |id),
#                          newdata = lb_train)


lp_train <- train2 %>%
  mutate(pred = predict(bks_lmer,
                        type = "response"))
plot2.5 <- ggplot() +
  geom_line(data = lp_train,
            aes(x = as.numeric(as.character(time)),
                y = ht,
                group = id),
            color = "black",
            alpha = 0.4) +
  geom_point(data = lp_train,
             aes(x = as.numeric(as.character(time)),
                y = ht,
                group = id),
             alpha = 0.5) +
  geom_point(data = lb_train,
            aes(x = as.numeric(as.character(time)), 
                y = pred, 
                group = id),
            color = "indianred") +
  # geom_point(data = lp_test,
  #           aes(x = as.numeric(as.character(time)),
  #               y = ht,
  #               group = id),
  #           color = "darkgreen") +
  # geom_point(data = data1_ci50,
  #            aes(x = as.numeric(as.character(time)),
  #               y = pred,
  #               group = id)) +
  facet_wrap(~id, ncol = 30) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(title = "Training Individual Trajectories",
       x = "Time",
       y = "Outcome Score")

plot2.5
ggsave("figures/P01_brokenstick_dynamic_comparison.png", 
       plot2.5, 
       width = 30,  
       height = 45, 
       units = "in", 
       dpi = 300) 
```

```{r}
knitr::include_graphics("figures/P01_brokenstick_dynamic_comparison.png")
```

```{r}
knitr::include_graphics("figures/P01_brokenstick_dynamic_comparison_time2.png")
```

## Step3 Matching

```{r}
#| eval: false

data2 <- lp_test %>%
  ungroup() %>%
  dplyr::select(id, time, ht) %>% 
  mutate(time = as.numeric(as.character(time)))

data1 <- lb_train1 %>% 
  ungroup() %>% 
  dplyr::select(id, time, ht = pred) %>% 
  mutate(time = as.numeric(as.character(time)))

subset <- data2 %>%
    group_by(id) %>%
    group_map(~ dis_match(lb_train = data1,
                          lb_test_ind = .,
                          train = train2,
                          match_methods = "mahalanobis", # "euclidean",
                          id_var = id,
                          outcome_var = ht,
                          time_var = time,
                          match_alpha = 0.95,
                          match_time = FALSE,
                          match_plot = TRUE),
              .progress = TRUE)

```

```{r}
#| eval: false

plots <- map(subset, "plot")

library(gridExtra)
ggsave("figures/P01_matching_subset_plot_mhl_0.95_time2.png", 
       marrangeGrob(grobs = plots,
                    ncol = 20, nrow = 23),
       width = 30,  
       height = 31, 
       units = "in", 
       dpi = 300)
```

Here is the matching subset individual trajectories for 
each target in the testing data.

I hope we can find or define the distance or the similarity 
ahead of the time and check whether it is the case in the simulation.
Any suggestions and ideas?


```{r}
knitr::include_graphics("figures/P01_matching_subset_plot_mhl_0.95_time2.png")
```

## Step4 Final prediction

```{r}
#| eval: false
gf <- "ht ~ cs(time, df = 3)"
gs <- "~ cs(time, df = 1)"
results <- test2 %>%
  group_by(id) %>%
  group_map(~ as.data.frame(.)) %>% 
  map2(subset, 
       ~try(predict_gamlss(matching = .y$subset,
                           test_one = .x,
                           id_var = id,
                           time_var = time,
                           outcome_var = ht,
                           tmin = 0,
                           tmax = 16,
                           gamlss_formula = gf,
                           gamsigma_formula = gs,
                           predict_plot = TRUE) %>% 
              suppressMessages()),
       .progress = TRUE)

# subset %>% length() # 457
save(results, subset,
     file = "figures/P01_gamlss_subset_results_data.Rdata")

#| eval: false
plots <- map(results, "predictive_centiles")

library(gridExtra)

ggsave("figures/P01_final_predictive_plot_095_anchor3_time2.png", 
       marrangeGrob(grobs = plots, ncol = 20, nrow = 25),
       width = 30, 
       height = 45, 
       dpi = 300)
```

```{r}
knitr::include_graphics("figures/P01_final_predictive_plot_095_anchor3_time2.png")
```

```{r}
load("figures/P01_gamlss_subset_results_data_p095_time2.Rdata")

meanout <- function(dataset,
                    term = c("bias", "mse", "coverage50",
                             "coverage80", "coverage90"),
                    ...){
  result <- dataset$centiles_observed %>%
    mutate(mse = bias^2) %>%
    dplyr::select(bias, mse, coverage50, coverage80, coverage90) %>%
    colMeans(na.rm = TRUE)
  
  return(result)
}

```


```{r}
summary_seed1 <- map_dfr(results, ~try(meanout(.,))) %>%
  mutate(rmse = sqrt(mse))
summary_seed1 %>% colMeans()
```








